%
%
function [result] = decision_tree(training_file, test_file, option, pruning_thr)
  %{
  % @params:
  %   - training_file: path to the training data file
  %   - test_file: path to the test file
  %   - option: can choose from multiple options. Read the assignment description for
  %       more details
  %   - pruning_thru: The pruning threshold
  %}
  try
    training_data = load(training_file);
    test_data = load(test_file);
  catch
    disp('Error: Unable to open training or/and test file');
    return;
  end
  if strcmp(option, 'optimized') == 1
    DTL_TopLevel(training_data, test_data, option, pruning_thr);
  elseif strcmp(option, 'randomized') == 1
    DTL_TopLevel(training_data, test_data, option, pruning_thr);
  elseif strcmp(option, 'forest3') == 1
    DFL_TopLevel(training_data, test_data, 3, 'randomized', pruning_thr);
  elseif strcmp(option, 'forest15') == 1
    DFL_TopLevel(training_data, test_data, 15, 'randomized', pruning_thr);
  else
    disp('Option not available.');
  end
end


function [result] = DTL_TopLevel(training_data, test_data, option, pruning_thr)
  %{
  % @params:
  %   - training_file: path to the training data file
  %   - test_file: path to the test file
  %   - option: can choose from multiple options. Read the assignment description for
  %       more details
  %   - pruning_thru: The pruning threshold
  %}
  num_cols = size(training_data, 2);
  default = mode(training_data(:, num_cols)); 
  unique_classes = unique(training_data(:, num_cols));
  tree = DTL(0, unique_classes, training_data, default, pruning_thr, option, 1);
  bfsSearch = bfsTreeSearch(0, tree, struct('bfs',[])); 
  sortedBfsSearch = sortrows(bfsSearch.bfs, 2);
  for i=1:size(sortedBfsSearch, 1)
    fprintf('tree=%2d, node=%3d, feature=%2d, thr=%6.2f, gain=%f\n', sortedBfsSearch(i,:));
  end
  [ntest_row, ntest_col] = size(test_data);
  classification_accuracy = 0;
  for i = 1:ntest_row
    true_class = test_data(i, ntest_col);
    temp_tree = tree;
    leaf_found = 0;
    probability = 0;
    accuracy = 0;
    predicted_class = -1;
    while leaf_found ~= 1
      if temp_tree.best_attribute ~= -1
        if test_data(i, temp_tree.best_attribute) < temp_tree.best_threshold
          temp_tree = temp_tree.leftChild;
        else
          temp_tree = temp_tree.rightChild;
        end
      else
        leaf_found = 1;
        probability = max(temp_tree.default);
        predicted_class = unique_classes(temp_tree.default == probability);
        ties = length(find(temp_tree.default == probability));
        if ties > 1
          num_predicted = size(predicted_class, 1);
          if sum(find(predicted_class == true_class)) == 1
            accuracy = 1/(num_predicted);
          end
          predicted_class = predicted_class(1,1);
        else
          if predicted_class == true_class
            accuracy = 1;
            classification_accuracy = classification_accuracy + 1;
          end
        end
      end
    end
   % fprintf('ID=%5d, predicted=%3d, true=%3d, accuracy=%4.2f\n', i-1, predicted_class, true_class, accuracy);
  end
  classification_accuracy = classification_accuracy/ntest_row;
  fprintf('classification accuracy=%6.4f\n', classification_accuracy);
end

function tree = DTL(tree_num, unique_classes, examples, default, pruning_thr, option, node_value)
  %{
  % @params:
  %   - tree_num: default to 0 for trees. Will be used when working with forests.
  %   - unique_classes: The set of unique classes in the dataset
  %   - examples: The training dataset.
  %   - default: The mode value of the training set. Used when there are < 2 unique classes
  %       or no example data
  %   - pruning_thr: The pruning threshold value
  %   - option: The 4 possible options as described in the assingment
  %   - node_value: The value assocaited with each node. 
  %}
  [example_nrows, example_ncols] = size(examples);
  max_gain = -1;
  best_attribute = -1;
  best_threshold = -1;
  num_classes = 0;
  if example_ncols > 0
    uniqueExampleClasses = unique(examples(:,example_ncols));
    num_classes = size(uniqueExampleClasses, 1);
  end
  if example_nrows < pruning_thr || num_classes == 1
    tree = struct('best_attribute', best_attribute, 'best_threshold', best_threshold, 'max_gain', max_gain, ...
      'node', node_value, 'default', default);
  else
    class_counter = zeros(size(unique_classes, 1), 1);
    if strcmp(option, 'optimized')
      [max_gain, best_attribute, best_threshold] = chooseAttributeOptimized(examples);
    else
      [max_gain, best_attribute, best_threshold] = chooseAttributeRandomized(examples);
    end
    tree = struct('best_attribute', best_attribute, 'best_threshold', best_threshold, ...
      'max_gain', max_gain, 'node', node_value);
    left = struct('examples',[], 'classes', class_counter);
    right = struct('examples',[], 'classes', class_counter); 
    for i = 1:example_nrows
      class_index = find(unique_classes == examples(i, example_ncols));
      if examples(i, best_attribute) < best_threshold
        left.examples=[left.examples; examples(i,:)];
        left.classes(class_index) = left.classes(class_index)+1;
      else 
        right.examples=[right.examples; examples(i,:)];
        right.classes(class_index) = right.classes(class_index)+1;
      end
    end

    nleft_examples = size(left.examples, 1);
    nright_examples = size(right.examples, 1);
    sumLeft = sum(left.classes(:,1));
    sumRight = sum(right.classes(:,1));

    if nleft_examples ~= 0 && nleft_examples > pruning_thr
      left.classes(:,1) = left.classes(:,1)/sumLeft;
    else
      left.classes = default;
    end

    if nright_examples ~= 0 && nright_examples > pruning_thr
      right.classes(:,1) = right.classes(:,1)/sumRight;
    else
      right.classes = default;
    end

    tree.leftChild = DTL(tree_num, unique_classes, left.examples,...
      left.classes, pruning_thr, option, node_value*2);

    tree.rightChild = DTL(tree_num, unique_classes, right.examples,...
      right.classes, pruning_thr, option, (node_value*2)+1);
  end
end 


function [max_gain, best_attribute, best_threshold] = chooseAttributeOptimized(examples)
  %{
  % @params:
  %   - examples: The training dataset
  %}
  max_gain = -1;
  best_attribute = -1;
  best_threshold = -1;
  numParent = size(examples, 1);
  num_attributes = size(examples, 2)-1;
  for i = 1:num_attributes
    attributes = examples(:, i);
    L = min(attributes);
    M = max(attributes);
    for k = 1:50
      threshold = L + k*(M-L)/51;
      % Calculate Information Gain.       
      gain = informationGain(examples, i, threshold, numParent);        
      if gain > max_gain
        max_gain = gain;
        best_attribute = i;
        best_threshold = threshold;
      end
    end
  end
end


function gain = informationGain(examples, n, threshold, numParent)
  %{
  % @params:
  %   - examples: The training dataset
  %   - n: the number of attributes (the nth attribute) 
  %   - Threshold: The threshold value calculated in the calling
  %       function
  %   - numParent: Number of parents
  %}

  % Total count of training examples
  [total_count, attr_num] = size(examples); 
  % Count the number of times the class has been outputted for 
  % the given examples
  unique_classes = unique(examples(:, attr_num));
  class_count = zeros(size(unique_classes,1), 1);
  K = struct('class_count', class_count, 'total', total_count);
  K_left = struct('class_count', class_count, 'total', 0);
  K_right = struct('class_count', class_count, 'total', 0);
  for i = 1:total_count
    % List the index of the unique class for that example
    index = find(unique_classes == examples(i, attr_num));
    K.class_count(index) = K.class_count(index)+1;
    if examples(i, n) < threshold
      K_left.total = K_left.total+1;
      K_left.class_count(index) = K_left.class_count(index)+1;
    else
      K_right.total = K_right.total+1;
      K_right.class_count(index) = K_right.class_count(index)+1;
    end
  end
  gain = calculateEntropy(K.class_count, numParent) - (K_left.total/numParent)* ...
    calculateEntropy(K_left.class_count, K_left.total) - (K_right.total/numParent)* ...
    calculateEntropy(K_right.class_count, K_right.total);
end 

function entropy = calculateEntropy(class_list, count)
  entropy = 0;
  num_classes = size(class_list, 1);
  for i = 1:num_classes
    if class_list(i) > 0
      entropy = entropy + ((-class_list(i)/count)*(log2(class_list(i)/count)));
    end
  end
end

function bfsSearch =  bfsTreeSearch(tree_num, tree, bfsSearch)
  node = [tree_num, tree.node, tree.best_attribute, tree.best_threshold, tree.max_gain];
  bfsSearch.bfs = [bfsSearch.bfs; node];
  if tree.best_attribute ~= -1
    bfsSearch = bfsTreeSearch(tree_num, tree.leftChild, bfsSearch);
    bfsSearch = bfsTreeSearch(tree_num, tree.rightChild, bfsSearch);
  end
end

function [max_gain, best_attribute, best_threshold] = chooseAttributeRandomized(examples)
  max_gain = -1;
  best_attribute = -1;
  best_threshold = -1;
  num_attributes = size(examples, 2);
  i = randi([1, num_attributes-1], 1, 1);
  attribute_values = examples(:,i);
  L = min(attribute_values);
  M = max(attribute_values);
  numParent = size(examples, 1);
  for k = 1:50
    threshold = L + k*(M-L)/51;
    % Calculate Information Gain.       
    gain = informationGain(examples, i, threshold, numParent);        
    if gain > max_gain
      max_gain = gain;
      best_attribute = i;
      best_threshold = threshold;
    end
  end
end


function [result] = DFL_TopLevel(training_data, test_data, numberOfTrees, option, pruning_thr)
  num_cols = size(training_data, 2);
  default = mode(training_data(:, num_cols)); 
  unique_classes = unique(training_data(:, num_cols));
  forest = struct('tree',[]);
  for i = 1:numberOfTrees
    forest.tree = [forest.tree ; DTL(i-1, unique_classes, training_data, default, pruning_thr, option, 1)];
  end
  for i = 1:numberOfTrees
    bfsSearch = bfsTreeSearch(i-1, forest.tree(i), struct('bfs', []));
    bfs = sortrows(bfsSearch.bfs, 2);
    for j = 1:size(bfs, 1)
      fprintf('tree=%2d, node=%3d, feature=%2d, thr=%6.2f, gain=%f\n', bfs(j,:));
    end
  end
  [ntest_row, ntest_col] = size(test_data);
  classification_accuracy = 0;
  for i = 1:ntest_row
    true_class = test_data(i, ntest_col);
    probability_tree = zeros(numberOfTrees, size(unique_classes, 1));
    for j = 1:numberOfTrees
      tree = forest.tree(j);
      accuracy = 0;
      leaf_node = 0;
      while leaf_node ~= 1
        if tree.best_attribute ~= -1
          if test_data(i, tree.best_attribute) < tree.best_threshold
            tree = tree.leftChild;
          else
            tree = tree.rightChild;
          end
        else
          leaf_node = 1;
          probability_tree(j,:) = tree.default';
        end
      end
    end
    probability_final = mean(probability_tree);
    probability = max(probability_final);
    predicted_class = unique_classes(probability_final == probability);
    ties = length(find(probability_final==probability));
    if ties > 1
      num_ties = size(predicted_class, 1);
      if sum(find(predicted_class==true_class)) == 1
        accuracy = 1/(num_ties);
      end
      predicted_class = predicted_class(1,1);
    else
      if predicted_class == true_class
        accuracy = 1;
        classification_accuracy = classification_accuracy+1;
      end
    end
   % fprintf('ID=%5d, predicted=%3d, true=%3d, accuracy=%4.2f\n', i-1, predicted_class, true_class, accuracy);
  end
  fprintf('classification accuracy=%6.4f\n', classification_accuracy/ntest_row);
end
